{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 1: Classical Generative Diffusion Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Deep Learning with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating tensors, transfer between CPU and GPU devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    Returns the available device ('cuda', 'mps', or 'cpu').\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    else:\n",
    "        return 'cpu'\n",
    "    \n",
    "device = get_device()\n",
    "print('Device in use:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simply creating tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a torch.tensor from a list\n",
    "x = torch.tensor([1, 2, 3])\n",
    "print(x)\n",
    "\n",
    "# create a torch.tensor from a numpy array is as straightforward\n",
    "import numpy as np\n",
    "x = np.array([1, 2, 3])\n",
    "x = torch.tensor(x)\n",
    "print(x)\n",
    "\n",
    "# also works with a list of lists\n",
    "x = [[1, 2], [3, 4]]\n",
    "x = torch.tensor(x)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensors operations** Typically element wise operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# element-wise operations\n",
    "x = torch.tensor([1, 2, 3]).to(device)\n",
    "y = torch.tensor([4, 5, 6]).to(device)\n",
    "add = x + y\n",
    "print('add', add)\n",
    "mul = x * y\n",
    "print('mul', mul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**stacking tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can stack multiple tensors together. See how the shape changes\n",
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.tensor([4, 5, 6])\n",
    "print('x xhape:', x.shape)\n",
    "print('y shape:', y.shape)\n",
    "z = torch.stack([x, y])\n",
    "print('z shape:', z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Special tensors: zero and one tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensor filled with zeros of dimension 2x3\n",
    "x = torch.zeros(2, 3)\n",
    "y = torch.ones(2, 3)\n",
    "print('x', x)\n",
    "print('y', y)\n",
    "\n",
    "# IMPORTANT : you can create a tensor filled with zeros with the SAME SHAPE as another tensor AND ON THE SAME DEVICE\n",
    "x = torch.tensor([1, 2, 3]).to(device)\n",
    "z = torch.zeros_like(x)\n",
    "print('z', z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Draw random variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a tensor from a uniform distribution\n",
    "x = torch.rand(2, 3)\n",
    "print('x', x)\n",
    "\n",
    "# Sample a tensor from a normal distribution\n",
    "y = torch.randn(2, 3)\n",
    "print('y', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example CPU vs GPU: Mandelbrot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a grid of complex numbers\n",
    "lim = 1.5\n",
    "x = torch.linspace(-lim, lim, 300)\n",
    "y = torch.linspace(-lim, lim, 300)\n",
    "X, Y = torch.meshgrid(x, y)\n",
    "C = X + 1j*Y\n",
    "C = C.to(device) # comment out to test on CPU\n",
    "print('C shape:', C.shape)\n",
    "\n",
    "# A point is in the Mandelbrot set if: z_{n+1} = z_n^2 + c does not diverge\n",
    "# We can use torch to compute the Mandelbrot set\n",
    "def mandelbrot(c, max_iter):\n",
    "    z = torch.zeros_like(c)\n",
    "    for _ in range(max_iter):\n",
    "        z = z*z + c\n",
    "    \n",
    "    # the point is in the Mandelbrot set if the absolute value of z is less than 2\n",
    "    in_mandelbrot = z.cpu().abs() < 2\n",
    "    return in_mandelbrot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C = C.to(device)\n",
    "Z = mandelbrot(C, 50)\n",
    "%timeit Z = mandelbrot(C, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the Mandelbrot set\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(Z.numpy(), extent=(-lim, lim, -lim, lim))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data: Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a dataset with a Gaussian mixture distribution:\n",
    "def get_gaussian_mixture_datapoints(mean1, mean2, std, n_samples):\n",
    "    print('Using Gaussian Mixture dataset, with parameters mean=[{}, {}], [{}, {}] and std={}. {} samples.'\n",
    "          .format(mean1[0], mean1[1], mean2[0], mean2[1], std, n_samples))\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def get_default_gaussian_mixture_datapoints():\n",
    "    mean1 = torch.tensor([-0.5, 0])\n",
    "    mean2 = torch.tensor([0.5, 0])\n",
    "    std = 0.1\n",
    "    n_samples = 10000\n",
    "    gaussian_datapoints = get_gaussian_mixture_datapoints(mean1, mean2, std, n_samples)\n",
    "    # shuffle the datapoints\n",
    "    gaussian_datapoints = gaussian_datapoints[torch.randperm(n_samples)]\n",
    "    return gaussian_datapoints\n",
    "\n",
    "gaussian_datapoints = get_default_gaussian_mixture_datapoints()\n",
    "\n",
    "# plot the Gaussian mixture dataset\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(gaussian_datapoints[:, 0], gaussian_datapoints[:, 1], s=1)\n",
    "plt.axis('equal')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Gaussian Mixture Dataset')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorDataset\n",
    "def create_dataset(datapoints):\n",
    "    labels = torch.zeros(datapoints.shape[0])\n",
    "    dataset_obj = torch.utils.data.TensorDataset(datapoints, labels)\n",
    "    return dataset_obj\n",
    "\n",
    "def create_dataloader(dataset, batch_size):\n",
    "    # Create a DataLoader\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True\n",
    "        )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "gaussian_datapoints = get_default_gaussian_mixture_datapoints()\n",
    "dataset_obj = create_dataset(gaussian_datapoints)\n",
    "dataloader = create_dataloader(dataset_obj, batch_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metric: assess the distance between two empirical distribution\n",
    "\n",
    "We will use the Wassertein-2 metric:\n",
    "$$W_2(\\mu, \\nu) = \\underset{\\gamma \\in \\mathcal{M}(\\mu, \\nu)}{\\inf} \\int \\| x - y \\|^2 \\gamma(dx, dy). $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyemd\n",
    "\n",
    "# Run emd_loss on two gaussian with different means and same std\n",
    "mean1 = torch.tensor([-0.5, 0])\n",
    "mean2 = torch.tensor([0.5, 0])\n",
    "std1 = 0.1\n",
    "std2 = 0.1\n",
    "n_samples = 10000\n",
    "gaussian_1 = torch.randn(n_samples, 2) * std1 + mean1\n",
    "gaussian_2 = torch.randn(n_samples, 2) * std2 + mean2\n",
    "\n",
    "# Compute the EMD between the two distributions\n",
    "emd = pyemd.emd_samples(gaussian_1, gaussian_2)\n",
    "\n",
    "print('Empirical EMD between the two distributions:', emd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Diffusion Process\n",
    "\n",
    "We need to define three functions:\n",
    "* **The forward diffusion**, i.e., sample $x_t$ given $x_0, t$. Since $p_{t |0}$ is available in closed form, we do not need to simulate a forward SDE; this is the *the simulation-free* property.\n",
    "* **The objective function**, which is the denoising squared $L_2$ loss.\n",
    "* **The sampling algorithm**, i.e., a simulation of the backward SDE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward : sample $p_{t | 0}$\n",
    "\n",
    "Implement two noise schedule:\n",
    "* **linear noise schedule** $\\beta_t$ scales linearly from $\\beta_{min} = 0.1$ to $\\beta_{max} = 20.0$.\n",
    "* **cosine noise schedule** Directly parameterize $\\bar \\alpha_t$ as \n",
    "$$\\bar \\alpha_t = \\cos(\\frac{\\bar t + s}{2(1 + s)} \\pi)^2,$$ \n",
    "where $s = 0.008$ and $\\bar  t = t / T$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def match_last_dims(data, shape):\n",
    "    \"\"\"\n",
    "    Repeat a 1D tensor so that its last dimensions [1:] match `size[1:]`.\n",
    "    Useful for working with batched data.\n",
    "    \"\"\"\n",
    "    assert len(data.shape) == 1, \"Data must be 1-dimensional (one value per batch)\"\n",
    "    for _ in range(len(shape) - 1):\n",
    "        data = data.unsqueeze(-1)\n",
    "    return data.repeat(1, *(shape[1:]))\n",
    "\n",
    "def compute_beta_t(t_norm, T, schedule = 'linear'):\n",
    "    # Compute β(t) depending on schedule.\n",
    "    \n",
    "    if schedule == 'linear':\n",
    "        ...\n",
    "    elif schedule == 'cosine':\n",
    "        s = 0.008\n",
    "        beta_t = (torch.pi / (T * (1 + s))) * torch.tan(((t_norm + s) / (1 + s)) * (torch.pi / 2))\n",
    "    else:\n",
    "        raise ValueError('Unknown schedule')\n",
    "    return beta_t\n",
    "\n",
    "def compute_alpha_bar(t_norm, schedule = 'linear'):\n",
    "    if schedule == 'linear': \n",
    "        ...\n",
    "    elif schedule == 'cosine':\n",
    "        alpha_bar = 0.5 * (1 - torch.cos(t_norm * torch.pi))\n",
    "        s = 0.008\n",
    "        alpha_bar = torch.cos((t_norm + s) / (1 + s) * (torch.pi / 2))**2\n",
    "    else:\n",
    "        raise ValueError('Unknown schedule')\n",
    "    return alpha_bar\n",
    "\n",
    "# must return x_t and the added noise, we will need it later to compute the loss\n",
    "def forward(x_start, t, T, schedule = 'linear'):\n",
    "    t_norm = t / T \n",
    "    alpha_bar = compute_alpha_bar(t_norm, schedule)\n",
    "    # expand alpha_bar to the same shape as x_start, so that we can multiply them\n",
    "    alpha_bar = match_last_dims(alpha_bar, x_start.shape)\n",
    "    noise = ...\n",
    "    x_t = ...\n",
    "    return x_t, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check the forward process by plotting the empirical marginals p_t, for some t and empirical samples\n",
    "x_start = dataset_obj[:1000][0]\n",
    "\n",
    "# create four subplots \n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "# plot the empirical marginals p_t\n",
    "def plot_ax_i(ax, x, y, title):\n",
    "    ax.scatter(x, y, s=1)\n",
    "    ax.axis('equal')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(title)\n",
    "\n",
    "T = 1\n",
    "\n",
    "for i, t_norm in enumerate([0.0, 0.25, 0.5, 1.0]):\n",
    "    t = t_norm * T * torch.ones_like(x_start)[:, 0]\n",
    "    x_t, _ = forward(x_start, t, T)\n",
    "    plot_ax_i(axs[i], x_t[:, 0], x_t[:, 1], 't = {}'.format(t[0]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Setting up the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.SimpleModel import MLPModel\n",
    "\n",
    "# define a simple MLP model. For the moment, take the default one I provide\n",
    "\n",
    "simple_model = MLPModel(\n",
    "    nfeatures = 2,\n",
    "    time_emb_size= 8,\n",
    "    nblocks = 2,\n",
    "    nunits = 32,\n",
    "    skip_connection = True,\n",
    "    layer_norm = True,\n",
    "    dropout_rate = 0.1,\n",
    "    learn_variance = False,\n",
    ")\n",
    "\n",
    "simple_model = simple_model.to(device)\n",
    "\n",
    "# setting up the optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    simple_model.parameters(), \n",
    "    lr=2e-3, \n",
    "    betas=(0.9, 0.999))\n",
    "\n",
    "# potentially set up a learning schedule too ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def training_losses(model, x_start, T):\n",
    "    \n",
    "    # Sample t uniformly from [0, T]\n",
    "    t = ... \n",
    "    x_t, noise = forward(x_start, t, T)\n",
    "    # The model takes x_t and t as input and predicts the noise. time t should be of shape (batch_size, 1)\n",
    "    # we can pass normalized time to model as input\n",
    "    t_norm = t / T\n",
    "    t_norm = t_norm.view(-1, 1)\n",
    "    predicted_noise = model(x_t, t_norm)\n",
    "    loss = ...\n",
    "    return loss\n",
    "\n",
    "# training loop\n",
    "\n",
    "import os\n",
    "\n",
    "def train(\n",
    "    num_epochs, \n",
    "    checkpoint_interval, \n",
    "    dataloader,\n",
    "    model,\n",
    "    optimizer,\n",
    "    checkpoint_dir,\n",
    "    device,\n",
    "    T = 1\n",
    "):\n",
    "    print(\"Training on device:\", device)\n",
    "\n",
    "    # Set the model to training mode.\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    for epoch in (range(1, num_epochs + 1)):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (data, _) in (enumerate(dataloader)):\n",
    "            data = data.to(device) \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute the training loss.\n",
    "            loss = training_losses(model, x_start=data, T=T)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        epoch_losses.append(avg_loss)\n",
    "        print(f\"Epoch [{epoch}] Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Save a checkpoint every checkpoint_interval epochs.\n",
    "        if epoch % checkpoint_interval == 0:\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}.pth\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch_losses': epoch_losses,\n",
    "            }, checkpoint_path)\n",
    "            print(\"Saved checkpoint to\", checkpoint_path)\n",
    "\n",
    "    print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN!\n",
    "train(\n",
    "    num_epochs=50,\n",
    "    checkpoint_interval=50,\n",
    "    dataloader=dataloader,\n",
    "    model=simple_model,\n",
    "    optimizer=optimizer,\n",
    "    checkpoint_dir='checkpoints',\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation: simulate the backward SDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def score_fn(model, x, t_norm):\n",
    "    \"\"\"\n",
    "    Given the noise-predicting model, returns the score (i.e. ∇_x log p_t(x))\n",
    "    at actual time t. Note that the model expects a normalized time (t/T).\n",
    "    For VP: score = - (predicted noise) / sqrt(1 - ᾱ(t))\n",
    "    \"\"\"\n",
    "    alpha_bar = ...\n",
    "    epsilon = ...\n",
    "    score = ...\n",
    "    return score\n",
    "\n",
    "def sample(\n",
    "    model,\n",
    "    n_samples,\n",
    "    reverse_steps,\n",
    "    schedule = 'linear',\n",
    "    T = 1):\n",
    "    \n",
    "    xt = torch.randn(...)\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Create a time discretization from T to 0\n",
    "        t_seq = ... \n",
    "        for i in tqdm(range(reverse_steps)):\n",
    "            t_current = t_seq[i]\n",
    "            t_next = t_seq[i + 1]\n",
    "            dt = t_next - t_current  # dt is negative (reverse time)\n",
    "            \n",
    "            # Create a batch of current time values for the update.\n",
    "            t_batch = ...\n",
    "            t_norm_batch = t_batch / T\n",
    "\n",
    "            \n",
    "            beta_t = compute_beta_t(t_norm_batch, T, schedule)\n",
    "            \n",
    "            f = ...\n",
    "            g = ...\n",
    "            \n",
    "            \n",
    "            # Get the score (using the noise-predicting network)\n",
    "            score = score_fn(model, xt, t_batch)\n",
    "            \n",
    "            # Euler–Maruyama update:\n",
    "            z = torch.randn_like(xt)\n",
    "            xt += ...\n",
    "            \n",
    "    return xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sample(\n",
    "    model=simple_model,\n",
    "    n_samples=1000,\n",
    "    reverse_steps=100,\n",
    "    schedule='linear',\n",
    "    T=1\n",
    ")\n",
    "samples = samples.cpu().detach().numpy()\n",
    "\n",
    "# plot samples\n",
    "tmp_samples = samples.clip(-1, 1)\n",
    "plt.scatter(tmp_samples[:, 0], tmp_samples[:, 1], s=1)\n",
    "plt.axis('equal')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-1, 1)\n",
    "plt.title('Generated Samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 1: SDE vs ODE sampling\n",
    "\n",
    "Modify the `sample` function to accept a `deterministic : bool` argument, according to which the sampling procedure will correspond to SDE or ODE sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(\n",
    "    model,\n",
    "    n_samples,\n",
    "    reverse_steps,\n",
    "    deterministic = False,\n",
    "    schedule = 'linear',\n",
    "    T = 1):\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sample(\n",
    "    model=simple_model,\n",
    "    n_samples=1000,\n",
    "    reverse_steps=20,\n",
    "    deterministic=True,\n",
    "    schedule='linear',\n",
    "    T=1\n",
    ")\n",
    "samples = samples.cpu().detach().numpy()\n",
    "\n",
    "# plot samples\n",
    "tmp_samples = samples.clip(-1, 1)\n",
    "plt.scatter(tmp_samples[:, 0], tmp_samples[:, 1], s=1)\n",
    "plt.axis('equal')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-1, 1)\n",
    "plt.title('Generated Samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a plot comparing the performance of SDE vs ODE sampling as a function of reverse steps, for example using the Wasserstein-2 metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now compare ODE vs SDE performance\n",
    "\n",
    "timesteps = [2, 5, 10, 20, 50, ]\n",
    "n_samples = 5000\n",
    "\n",
    "samples_sde = ...\n",
    "\n",
    "samples_ode = ...\n",
    "\n",
    "# retrieve true samples from dataset\n",
    "true_samples = dataset_obj[:n_samples][0].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute emd distance between samples\n",
    "emd_sde = [pyemd.emd_samples(true_samples, samples.detach().cpu().numpy()) for samples in samples_sde]\n",
    "emd_ode = [pyemd.emd_samples(true_samples, samples.detach().cpu().numpy()) for samples in samples_ode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now plot the EMD distance as a function of the number of reverse steps\n",
    "plt.plot(timesteps, emd_sde, label='SDE')\n",
    "plt.plot(timesteps, emd_ode, label='ODE')\n",
    "plt.xlabel('Number of reverse steps')\n",
    "plt.ylabel('EMD distance')\n",
    "plt.legend()\n",
    "plt.title('EMD distance as a function of the number of reverse steps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 2: Conditioning with Classifier-Free Guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    Returns the available device ('cuda', 'mps', or 'cpu').\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    else:\n",
    "        return 'cpu'\n",
    "    \n",
    "device = get_device()\n",
    "print('Device in use:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify the Dataset** \n",
    "\n",
    "Integrate class labels $y \\in \\{0, 1\\}$ in Gaussian 2-mixture dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a dataset with a Gaussian mixture distribution:\n",
    "def get_gaussian_mixture_datapoints(mean1, mean2, std, n_samples):\n",
    "    ...\n",
    "    return samples, labels\n",
    "\n",
    "def get_default_gaussian_mixture_datapoints():\n",
    "    mean1 = torch.tensor([-0.5, 0])\n",
    "    mean2 = torch.tensor([0.5, 0])\n",
    "    std = 0.1\n",
    "    n_samples = 10000\n",
    "    gaussian_datapoints, labels = get_gaussian_mixture_datapoints(mean1, mean2, std, n_samples)\n",
    "    # shuffle the data\n",
    "    perm = torch.randperm(n_samples)\n",
    "    gaussian_datapoints = gaussian_datapoints[perm]\n",
    "    labels = labels[perm]\n",
    "    return gaussian_datapoints, labels\n",
    "\n",
    "# random shuffle\n",
    "gaussian_datapoints, labels = get_default_gaussian_mixture_datapoints()\n",
    "\n",
    "# plot the Gaussian mixture dataset\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(gaussian_datapoints[labels == 0, 0], gaussian_datapoints[labels == 0, 1], s=1, label='Class 0')\n",
    "plt.scatter(gaussian_datapoints[labels == 1, 0], gaussian_datapoints[labels == 1, 1], s=1, label='Class 1')\n",
    "plt.axis('equal')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('Gaussian Mixture Dataset')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Create a TensorDataset\n",
    "def create_dataset(datapoints, labels):\n",
    "    dataset_obj = torch.utils.data.TensorDataset(datapoints, labels)\n",
    "    return dataset_obj\n",
    "\n",
    "def create_dataloader(dataset, batch_size):\n",
    "    # Create a DataLoader\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True\n",
    "        )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "gaussian_datapoints, labels = get_default_gaussian_mixture_datapoints()\n",
    "dataset_obj = create_dataset(gaussian_datapoints, labels)\n",
    "dataloader = create_dataloader(dataset_obj, batch_size=500)\n",
    "\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify the neural network** \n",
    "* It should accept all possible class labels $y$, plus the null class label $\\emptyset$, which will correspond to the *unconditional* label. It can be represented by any value you like.\n",
    "* Try to use `nn.Embedding`; the model will learn to embed class labels in $\\mathbb{R}^d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model.SimpleModelConditioned as SimpleModelConditioned\n",
    "\n",
    "simple_model_conditioned = SimpleModelConditioned.MLPModel(\n",
    "    nfeatures = 2,\n",
    "    time_emb_size=8,\n",
    "    nblocks = 2,\n",
    "    nunits = 32,\n",
    "    skip_connection = True,\n",
    "    layer_norm = True,\n",
    "    dropout_rate = 0.1,\n",
    "    num_classes = 2\n",
    ")\n",
    "\n",
    "simple_model_conditioned = simple_model_conditioned.to(device)\n",
    "\n",
    "# setting up the optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    simple_model_conditioned.parameters(), \n",
    "    lr=2e-3, \n",
    "    betas=(0.9, 0.999))\n",
    "\n",
    "# potentially set up a learning schedule too ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def training_losses(model, x_start, T, y):\n",
    "    ...\n",
    "    return loss\n",
    "\n",
    "# training loop\n",
    "\n",
    "import os\n",
    "\n",
    "def train(\n",
    "    num_epochs, \n",
    "    checkpoint_interval, \n",
    "    dataloader,\n",
    "    model,\n",
    "    optimizer,\n",
    "    checkpoint_dir,\n",
    "    device,\n",
    "    T = 1\n",
    "):\n",
    "    print(\"Training on device:\", device)\n",
    "\n",
    "    # Set the model to training mode.\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    for epoch in (range(1, num_epochs + 1)):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (data, y) in (enumerate(dataloader)):\n",
    "            data = data.to(device) \n",
    "            y = y.to(device)\n",
    "            # 10% of the time, set y to null label (for instance, null label = max_num_classes + 1)\n",
    "            y[torch.rand_like(y) < 0.1] = 2\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute the training loss.\n",
    "            loss = training_losses(model, x_start=data, T=T, y = y.int()) # y must be an integer tensor if using nn.Embedding\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        epoch_losses.append(avg_loss)\n",
    "        print(f\"Epoch [{epoch}] Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Save a checkpoint every checkpoint_interval epochs.\n",
    "        if epoch % checkpoint_interval == 0:\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}.pth\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch_losses': epoch_losses,\n",
    "            }, checkpoint_path)\n",
    "            print(\"Saved checkpoint to\", checkpoint_path)\n",
    "\n",
    "    print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN!\n",
    "train(\n",
    "    num_epochs=50,\n",
    "    checkpoint_interval=50,\n",
    "    dataloader=dataloader,\n",
    "    model=simple_model_conditioned,\n",
    "    optimizer=optimizer,\n",
    "    checkpoint_dir='checkpoints_condtioned',\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify the sampling algorithm** \n",
    "\n",
    "It should accept `guidance_scale` as argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def score_fn(model, x, t_norm, y):\n",
    "    \"\"\"\n",
    "    Given the noise-predicting model, returns the score (i.e. ∇_x log p_t(x))\n",
    "    at actual time t. Note that the model expects a normalized time (t/T).\n",
    "    For VP: score = - (predicted noise) / sqrt(1 - ᾱ(t))\n",
    "    \"\"\"\n",
    "\n",
    "    alpha_bar = ...\n",
    "    epsilon = ...\n",
    "    score = ...\n",
    "    return score\n",
    "\n",
    "def sample(\n",
    "    model,\n",
    "    n_samples,\n",
    "    reverse_steps,\n",
    "    class_label,\n",
    "    deterministic = False,\n",
    "    guidance_scale = 3.0,\n",
    "    schedule = 'linear',\n",
    "    T = 1):\n",
    "    \n",
    "    xt = ...\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Create a time discretization from T to 0\n",
    "        t_seq = ...\n",
    "        for i in tqdm(range(reverse_steps)):\n",
    "            \n",
    "            ...\n",
    "            \n",
    "            \n",
    "            # Get the score (using the noise-predicting network)\n",
    "            \n",
    "            score_cond = score_fn(..., class_label)\n",
    "            \n",
    "            uncond_class_label = ...\n",
    "            score_uncond = score_fn(..., uncond_class_label)\n",
    "            \n",
    "            score = ... # include guidance\n",
    "            \n",
    "            \n",
    "            if deterministic:\n",
    "                ...\n",
    "            else:\n",
    "                ...\n",
    "            \n",
    "    return xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "\n",
    "Visually observe what happens with increasing guidance scale. Quantify with Wasserstein metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label = 0\n",
    "guidance_scale = 1\n",
    "samples = sample(\n",
    "    model=simple_model_conditioned,\n",
    "    n_samples=1000,\n",
    "    reverse_steps=20,\n",
    "    class_label=class_label,\n",
    "    guidance_scale=guidance_scale,\n",
    "    deterministic=False,\n",
    "    schedule='linear',\n",
    "    T=1\n",
    ")\n",
    "samples = samples.cpu().detach().numpy()\n",
    "\n",
    "# plot samples\n",
    "tmp_samples = samples.clip(-2, 2)\n",
    "plt.scatter(tmp_samples[:, 0], tmp_samples[:, 1], s=1)\n",
    "plt.axis('equal')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-1, 1)\n",
    "plt.title('Generated Samples with class {}, guidance {}'.format(class_label, guidance_scale))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go further: Elucidated Diffusion Model \n",
    "\n",
    "(Elucidating the Design Space of Diffusion Model)\n",
    "\n",
    "Discuss the paper with your colleagues or with me. Implement the recommend design choices, in terms of: \n",
    "* Sampling\n",
    "* Network and pre-conditioning\n",
    "* Training\n",
    "\n",
    "All these design choices will require choosing the right hyper-parameters to be chosen for the working dataset. \n",
    "\n",
    "Typically, one does not want to go through all these troubles... there is a reason why people have settled on a default choice:\n",
    "* the VP process \n",
    "* epsilon-prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
